From: Gordon Malm <bugs-gentoo-org-02@bumpin.org>

Backport of various fixes for vma mirroring bugs in SEGMEXEC from 2.6.24
branch. Closes gentoo bug 198051.

These patches are present in upstream grsecurity patches as of
pax-linux-2.6.24.2-test29.patch. This patch can be dropped for any
hardened-sources-2.6.24 based upon pax-linux-2.6.24.2-test29.patch or
later.

Acked-by: Kerin Millar <kerframil@gmail.com>

diff -urP linux-2.6.23-hardened-r7-orig/mm/memory.c linux-2.6.23-hardened-r7-allfixes-r2/mm/memory.c
--- linux-2.6.23-hardened-r7-orig/mm/memory.c
+++ linux-2.6.23-hardened-r7-allfixes-r2/mm/memory.c
@@ -1777,13 +1777,13 @@
 	pte_unmap_nested(pte_m);
 }
 
-static void pax_mirror_pte(struct vm_area_struct *vma, unsigned long address, pte_t *pte, spinlock_t *ptl)
+static void pax_mirror_pte(struct vm_area_struct *vma, unsigned long address, pte_t *pte, pmd_t *pmd, spinlock_t *ptl)
 {
 	struct page *page_m;
 	pte_t entry;
 
 	if (!(vma->vm_mm->pax_flags & MF_PAX_SEGMEXEC))
-		return;
+		goto out;
 
 	entry = *pte;
 	page_m  = vm_normal_page(vma, address, entry);
@@ -1791,9 +1791,9 @@
 		pax_mirror_pfn_pte(vma, address, pte_pfn(entry), ptl);
 	else if (PageAnon(page_m)) {
 		if (pax_find_mirror_vma(vma)) {
-			spin_unlock(ptl);
+			pte_unmap_unlock(pte, ptl);
 			lock_page(page_m);
-			spin_lock(ptl);
+			pte = pte_offset_map_lock(vma->vm_mm, pmd, address, &ptl);
 			if (pte_same(entry, *pte))
 				pax_mirror_anon_pte(vma, address, page_m, ptl);
 			else
@@ -1801,6 +1801,9 @@
 		}
 	} else
 		pax_mirror_file_pte(vma, address, page_m, ptl);
+
+out:
+	pte_unmap_unlock(pte, ptl);
 }
 #endif
 
@@ -2871,7 +2874,8 @@
 	}
 
 #ifdef CONFIG_PAX_SEGMEXEC
-	pax_mirror_pte(vma, address, pte, ptl);
+	pax_mirror_pte(vma, address, pte, pmd, ptl);
+	return 0;
 #endif
 
 unlock:
diff -urP linux-2.6.23-hardened-r7-orig/mm/mmap.c linux-2.6.23-hardened-r7-allfixes-r2/mm/mmap.c
--- linux-2.6.23-hardened-r7-orig/mm/mmap.c
+++ linux-2.6.23-hardened-r7-allfixes-r2/mm/mmap.c
@@ -877,6 +877,19 @@
 			if (area_m)
 				vma_adjust(area_m, addr_m, next_m->vm_end,
 					next_m->vm_pgoff - pglen, NULL);
+			else if (next_m) {
+				vma_adjust(next_m, addr_m, next_m->vm_end,
+					next_m->vm_pgoff - pglen, NULL);
+				BUG_ON(area == next);
+				BUG_ON(area->vm_mirror);
+				BUG_ON(next_m->anon_vma && next_m->anon_vma != area->anon_vma);
+				area->vm_mirror = next_m;
+				next_m->vm_mirror = area;
+				if (area->anon_vma && !next_m->anon_vma) {
+					next_m->anon_vma = area->anon_vma;
+					anon_vma_link(next_m);
+				}
+			}
 #endif
 
 		}
@@ -1244,9 +1257,8 @@
 	if ((mm->pax_flags & MF_PAX_SEGMEXEC) && (vm_flags & VM_EXEC)) {
 		vma_m = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 		if (!vma_m) {
-			kmem_cache_free(vm_area_cachep, vma);
 			error = -ENOMEM;
-			goto unacct_error;
+			goto free_vma;
 		}
 	}
 #endif
@@ -1274,6 +1286,19 @@
 		if (error)
 			goto unmap_and_free_vma;
 
+#ifdef CONFIG_PAX_SEGMEXEC
+		if (vma_m) {
+			struct mempolicy *pol;
+
+			pol = mpol_copy(vma_policy(vma));
+			if (IS_ERR(pol)) {
+				mpol_free(vma_policy(vma));
+				goto unmap_and_free_vma;
+			}
+			vma_set_policy(vma_m, pol);
+		}
+#endif
+
 #if defined(CONFIG_PAX_PAGEEXEC) && defined(CONFIG_X86_32)
 		if ((mm->pax_flags & MF_PAX_PAGEEXEC) && !(vma->vm_flags & VM_SPECIAL)) {
 			vma->vm_flags |= VM_PAGEEXEC;
@@ -1328,6 +1353,14 @@
 		mpol_free(vma_policy(vma));
 		kmem_cache_free(vm_area_cachep, vma);
 		vma = NULL;
+
+#ifdef CONFIG_PAX_SEGMEXEC
+		if (vma_m) {
+			mpol_free(vma_policy(vma_m));
+			kmem_cache_free(vm_area_cachep, vma_m);
+		}
+#endif
+
 	}
 out:	
 	mm->total_vm += len >> PAGE_SHIFT;
@@ -2539,6 +2572,8 @@
 	struct rb_node **rb_link, *rb_parent;
 	struct mempolicy *pol;
 
+	BUG_ON(vma->vm_mirror);
+
 	/*
 	 * If anonymous vma has not yet been faulted, update new pgoff
 	 * to match new location, to increase its chance of merging.
@@ -2584,10 +2619,14 @@
 {
 	struct vm_area_struct *prev_m;
 	struct rb_node **rb_link_m, *rb_parent_m;
+	struct mempolicy *pol_m;
 
 	BUG_ON(!(vma->vm_mm->pax_flags & MF_PAX_SEGMEXEC) || !(vma->vm_flags & VM_EXEC));
-	BUG_ON(vma->vm_mirror || vma_m->vm_mirror || vma_policy(vma));
+	BUG_ON(vma->vm_mirror || vma_m->vm_mirror);
+	BUG_ON(!vma_mpol_equal(vma, vma_m));
+	pol_m = vma_policy(vma_m);
 	*vma_m = *vma;
+	vma_set_policy(vma_m, pol_m);
 	vma_m->vm_start += SEGMEXEC_TASK_SIZE;
 	vma_m->vm_end += SEGMEXEC_TASK_SIZE;
 	vma_m->vm_flags &= ~(VM_WRITE | VM_MAYWRITE | VM_ACCOUNT | VM_LOCKED);
diff -urP linux-2.6.23-hardened-r7-orig/mm/mprotect.c linux-2.6.23-hardened-r7-allfixes-r2/mm/mprotect.c
--- linux-2.6.23-hardened-r7-orig/mm/mprotect.c
+++ linux-2.6.23-hardened-r7-allfixes-r2/mm/mprotect.c
@@ -208,6 +208,8 @@
 			error = split_vma(mm, vma, start, 1);
 			if (error)
 				return -ENOMEM;
+			BUG_ON(!*pprev || (*pprev)->vm_next == vma);
+			*pprev = (*pprev)->vm_next;
 		}
 
 		if (end != vma->vm_end) {
@@ -266,11 +268,20 @@
 
 #ifdef CONFIG_PAX_SEGMEXEC
 	if ((mm->pax_flags & MF_PAX_SEGMEXEC) && !(oldflags & VM_EXEC) && (newflags & VM_EXEC)) {
+		struct mempolicy *pol;
+
 		vma_m = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 		if (!vma_m) {
 			error = -ENOMEM;
 			goto fail;
 		}
+		pol = mpol_copy(vma_policy(vma));
+		if (IS_ERR(pol)) {
+			kmem_cache_free(vm_area_cachep, vma_m);
+			error = -ENOMEM;
+			goto fail;
+		}
+		vma_set_policy(vma_m, pol);
 	}
 #endif
 
